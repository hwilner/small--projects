{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bGD with early stoping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7RUPN0z2SNh",
        "colab_type": "text"
      },
      "source": [
        "add lasso and ridge\n",
        "#no skl learn :/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKN6dDeX2u7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80um07Ah2ysy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR:\n",
        "  def __init__(self  ,degree = 10, fit_by=1 , learning_rate=1 ,epsilon=1E-3 ,earlystoping= True , tol=1E-3 , patience= 5 , val=0.2 , alpha=1 , r=0 ):\n",
        "    ''' function will do linear regression for data, it recive, degree (1),  and hot to fit polynimal (2), furier base or mixsed(3)\n",
        "        args:\n",
        "          degree - rank of polinomal / fourier base series \n",
        "          fit_by - how to do the polynomal fit \n",
        "          \n",
        "    '''\n",
        "    \n",
        "    self.epsilon=epsilon\n",
        "    self.fit_by=fit_by\n",
        "    self.learning_rate=learning_rate\n",
        "    self.DEGREE = degree*fit_by\n",
        "#     self.cost = cost\n",
        "    self.earlystoping= earlystoping\n",
        "    self.tol=tol\n",
        "    self.patience= patience\n",
        "    self.val = val # between 0-1\n",
        "    self.alpha= alpha\n",
        "    self.r=r\n",
        "    \n",
        "    \n",
        "#     self.weigth maybe we can start with set weigth and update them ?\n",
        "    # we need to normilize the x values to predict\n",
        "    # ymin is b in the function\n",
        "   \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  def normalize(self, X, y=None):\n",
        "    ''' transfering the data to  0-1 range, keeping normlization value\n",
        "    args:\n",
        "      X-  range of fiting numpy aray\n",
        "      values prdicted or to predict by- len of x\n",
        "    \n",
        "    atributes :\n",
        "      self.miny -  min of y predicted value\n",
        "      self.minx  - min x of x given\n",
        "      self.maxy  - max y of predicted value\n",
        "      self.maxx  - max x of prdeicted value \n",
        "      \n",
        "       \n",
        "    \n",
        "    \n",
        "    '''\n",
        "    if y is not None:\n",
        "      self.miny = np.min(y)\n",
        "      y -= self.miny\n",
        "      self.maxy = np.max(y)\n",
        "      y /= self.maxy      \n",
        "      self.minx = np.min(X)\n",
        "      self.maxx = np.max(X)\n",
        "    X -= self.minx\n",
        "    \n",
        "    X /= (self.maxx-self.minx)\n",
        "    \n",
        "    if y is not None:\n",
        "      return X.reshape(-1,1), y.reshape(-1,1)\n",
        "    else:\n",
        "      return X.reshape(-1,1)\n",
        "    \n",
        "    \n",
        "  def un_norm(self,X, y, ) :\n",
        "    ''' transfering the data from  0-1 range to the predicted range\n",
        "    args:\n",
        "      X-  range of fiting numpy aray\n",
        "      y-values prdicted or to predict by- len of x\n",
        "    \n",
        "    atributes :\n",
        "     \n",
        "    return: \n",
        "      X,y in original scale \n",
        "      \n",
        "       \n",
        "    \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    y*=self.maxy + self.miny\n",
        "    X*=(self.maxx + self.minx)+self.minx\n",
        "    return y\n",
        "  \n",
        "  \n",
        "  def choose_model(self,xnew):\n",
        "    exp=np.arange(self.DEGREE+1)\n",
        "    # use meaningful names instead of a & b like x_col and exp_col\n",
        "    # Or reshape \"in place\"\n",
        "    \n",
        "    a=xnew.reshape(-1,1) #a.shape (1000, 1)\n",
        "\n",
        "\n",
        "#     print('a.shape', a.shape)\n",
        "    \n",
        "    b=exp.reshape(1,-1)\n",
        "#     print('b.shape', b.shape) b.shape (1, 11)\n",
        "    \n",
        "    pol=a**b\n",
        "#     print( 'pol ' ,pol.shape)  pol  (1000, 11)\n",
        "    f_base= np.hstack((  np.ones(len(a)).reshape(-1,1) , np.cos(2*np.pi *b*a) ,  np.sin(2*np.pi *b*a))  )\n",
        "    model = np.hstack (( self.fit_by%2*pol, self.fit_by//2*f_base))\n",
        "    \n",
        "#     self.model =model[ ~np.all(model == 0, axis=0)] # maybe axis 1?\n",
        "    # Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 6 is different from 19)\n",
        "    if self.fit_by == 1:\n",
        "      self.model=pol\n",
        "    elif self.fit_by == 2 :\n",
        "      self.model=f_base\n",
        "    else:\n",
        "      self.model=model\n",
        "      \n",
        "  \n",
        "    \n",
        "    \n",
        "  def predict (self, x ):\n",
        "#     ypred = np.matmul(self.model ,self.weigths )\n",
        "    xnorm = self.normalize ( x )\n",
        "    ypred=np.matmul(self.model,self.weigths)\n",
        "    print(self.weigths)\n",
        "    \n",
        "   \n",
        "    return self.un_norm(xnorm,ypred)\n",
        "    \n",
        "  def fit (self, X, y)   :\n",
        "    \n",
        "    self.weigths = np.random.uniform(-0.1,0.1, self.fit_by * self.DEGREE +1).reshape(-1,1) #(11,1)\n",
        "    Xnor , ynor =self.normalize(X, y) # (1000,1)\n",
        "    r= self.r\n",
        "    alpha =self.alpha\n",
        "    #shufle and split no sklearn  \n",
        "    indices = np.arange(y.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    X = Xnor[indices]\n",
        "    y = ynor[indices]\n",
        "    \n",
        "    #split\n",
        "    l=len(y)\n",
        "    ii= int(self.val *l)\n",
        "    x_val ,y_val = X[:ii] , y[:ii]\n",
        "    x_train, y_train = X[ ii: ] , y[ii :]\n",
        "    self.choose_model(x_train)\n",
        "    # start with train \n",
        "    i=0\n",
        " \n",
        "    w_old=self.weigths.copy()\n",
        "    train_contin =True\n",
        "    while True:   # we want to cuntinue until break\n",
        "      i+=1\n",
        "      pred = np.matmul( self.model ,self.weigths )\n",
        "      delta = y_train - pred\n",
        "      derr =- delta * self.model #error derivative   \n",
        "      av_d = derr.mean(axis=0).reshape(-1,1)\n",
        "      w= self.weigths\n",
        "      self.weigths -= av_d  + r*alpha*w + (alpha*(1-r)/2)*w\n",
        "      if i%2000==0:\n",
        "        w_old=self.weigths\n",
        "        w_new=self.weigths.copy()\n",
        "      \n",
        "\n",
        "        if np.sum(( self.weigths-w_old ) **2) < self.epsilon:  #equel to error\n",
        "          train_contin = False\n",
        "        else:\n",
        "          w_old = self.weigths.copy()\n",
        "      \n",
        "      \n",
        "    i=0\n",
        "    self.choose_model(x_val)\n",
        "    w_old=self.weigths.copy()\n",
        "    train_contin =True\n",
        "    patience_left = sel.patience  +1\n",
        "    while (patience_left>= 0) or (train_contin) :   # we want to cuntinue until break\n",
        "      patience_left-=1\n",
        "      pred = np.matmul( self.model ,self.weigths )\n",
        "      delta = y_val - pred\n",
        "      derr =- delta * self.model #error derivative   \n",
        "      av_d = derr.mean(axis=0).reshape(-1,1)\n",
        "      w= self.weigths\n",
        "      self.weigths -= av_d  + r*alpha*w + (alpha*(1-r)/2)*w\n",
        "      mses = (delta**2).mean(axis=1)\n",
        "      if patience_left==1 and mses<self.tol:\n",
        "        break\n",
        "      if i%2000==0:\n",
        "        w_old=self.weigths\n",
        "        w_new=self.weigths.copy()\n",
        "    \n",
        "\n",
        "        if np.sum(( self.weigths-w_old ) **2) < self.epsilon:  #equel to error\n",
        "          train_contin = False\n",
        "        else:\n",
        "          w_old = self.weigths.copy()\n",
        "    \n",
        "\n",
        "  \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xKsx-9oAo_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "SAMPLE_COUNT = int(1E3)\n",
        "COEF = 1E-2\n",
        "a = np.random.rand(SAMPLE_COUNT);\n",
        "b = np.sin(COEF * np.cumsum(1 / (a - a.mean())))\n",
        "c = np.cumsum(b - b.mean())\n",
        "NOISE = 2E-1\n",
        "y = c + NOISE * np.random.rand(SAMPLE_COUNT)\n",
        "y /= np.random.rand()\n",
        "y -= 1 / np.random.rand()\n",
        "X = np.arange(SAMPLE_COUNT) + np.random.rand(SAMPLE_COUNT)\n",
        "X /= np.random.rand() * SAMPLE_COUNT\n",
        "X -= 1 / np.random.rand()\n",
        "\n",
        "x=LR(degree = 3,fit_by=1 , learning_rate=1 ,epsilon=1E-3,earlystoping= True , tol=1E-3 , patience=5 , val=0.2 , alpha=0 , r=0.5)\n",
        "x.fit(X,y)\n",
        "y_pred=x.predict(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXTc9AhPB9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}